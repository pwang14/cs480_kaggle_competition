{"cells":[{"cell_type":"markdown","source":["# To run the notebook\n","\n","Use Google Colab (should try to use GPU since training is slow without it). Before running any code, upload *train.csv* and *test.csv* to the filesystem first. Then run all the cells. \n","\n","After the last cell runs, two files called *desc_train_pred.csv* and *desc_test_pred.csv* should be saved to the filesystem. Download these files since they're required for catboost.ipynb.\n","\n","# Summary of techniques\n","\n","For this notebook I only used the noisy text descriptions.\n","\n","To process the data, I first created a mapping of all the unique words in the text descriptions to numerical values. I created training and validation datasets, where I would return the text description as a fixed length vector of numerical mappings (with padding at the end if necessary).\n","\n","I used a transformer to analyze the text descriptions. For a given input vector, I first turn it into an embedding. However, I omit any positional embedding since the text descriptions seem more like bags of words to me (so the position doesn't seem relevant). I then run the embedding through the transformer, making sure to mask the padding. I also use a single padding value for the target vector.\n","\n","I originally used just a transformer encoder, followed by some linear layers. I did this since I figured I wouldn't need a decoder if I'm just classifying text instead of generating a sequence.\n","\n","I then trained the network and generated my predictions, which would go to catboost.ipynb.\n","\n","I experimented with many different hyperparameters. I think just using the transformer encoder gave better results, but I'm not entirely sure (might've just gotten unlucky with my hyperparameters).\n"],"metadata":{"id":"vW9c6NoWbTjv"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"nEgwCmZIBLXS","executionInfo":{"status":"ok","timestamp":1682199449169,"user_tz":240,"elapsed":5693,"user":{"displayName":"Patrick Wang","userId":"09514208081734500853"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","from torch import Tensor\n","from torch.nn import Transformer, TransformerEncoder, TransformerEncoderLayer\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1682199449170,"user":{"displayName":"Patrick Wang","userId":"09514208081734500853"},"user_tz":240},"id":"rQXLuj829Cof","outputId":"b31284ee-35d7-4988-86d7-e833f1ba93b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":451,"status":"ok","timestamp":1682199449617,"user":{"displayName":"Patrick Wang","userId":"09514208081734500853"},"user_tz":240},"id":"-_-zqgbJ9Cog","outputId":"e0ec872e-e606-4526-cbce-d2867839a3b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["14\n","[1211, 3975, 15, 16, 64, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}],"source":["df = pd.read_csv('train.csv')[['category', 'noisyTextDescription']]\n","df_test = pd.read_csv('test.csv')\n","\n","c_to_i = {}\n","i_to_c = {}\n","for i, c in enumerate(df['category'].unique()):\n","    c_to_i[c] = i\n","    i_to_c[i] = c\n","\n","num_categories = len(c_to_i)\n","\n","text_mapping = {\n","    \"<PAD>\": 0\n","}\n","max_desc_len = 0\n","for desc in df['noisyTextDescription']:\n","    tokens = desc.split()\n","    max_desc_len = max(max_desc_len, len(tokens))\n","\n","    for word in tokens:\n","        if word not in text_mapping:\n","            text_mapping[word] = len(text_mapping)\n","\n","for desc in df_test['noisyTextDescription']:\n","    tokens = desc.split()\n","    max_desc_len = max(max_desc_len, len(tokens))\n","\n","    for word in tokens:\n","        if word not in text_mapping:\n","            text_mapping[word] = len(text_mapping)\n","\n","num_words = len(text_mapping)\n","print(max_desc_len)\n","\n","def convert_text(text):\n","    tokens = [text_mapping[word] for word in text.split()]\n","    for _ in range(len(tokens), max_desc_len):\n","        tokens.append(0)\n","    return tokens\n","\n","df['noisyTextDescription'] = df['noisyTextDescription'].apply(convert_text)\n","df['category'] = df['category'].apply(lambda x: c_to_i[x])\n","\n","df = df.sample(frac=1).reset_index(drop=True)\n","\n","print(df['noisyTextDescription'].iloc[0])\n","\n","df_train, df_val = train_test_split(df, train_size=0.8)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ru-LtB2O9dE7","executionInfo":{"status":"ok","timestamp":1682199449617,"user_tz":240,"elapsed":5,"user":{"displayName":"Patrick Wang","userId":"09514208081734500853"}}},"outputs":[],"source":["class DescriptionData(Dataset):\n","    def __init__(self, df, test=False):\n","        self.df = df\n","        self.len = len(self.df)\n","        self.test = test\n","\n","    def __getitem__(self, idx):\n","        desc = torch.tensor(self.df['noisyTextDescription'].iloc[idx]).to(device)\n","        \n","        if self.test:\n","            return desc\n","        else:\n","            category = self.df['category'].iloc[idx]\n","            return desc, category\n","\n","    def __len__(self):\n","        return self.len\n","\n","train_dataset = DescriptionData(df_train)\n","val_dataset = DescriptionData(df_val)\n","total_dataset = DescriptionData(df)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n","total_dataloader = DataLoader(total_dataset, batch_size=64, shuffle=True)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"RYBeoppW9Coi","executionInfo":{"status":"ok","timestamp":1682199449618,"user_tz":240,"elapsed":5,"user":{"displayName":"Patrick Wang","userId":"09514208081734500853"}}},"outputs":[],"source":["class DescriptionTransformer(nn.Module):\n","    def __init__(self,\n","                 num_encoder_layers: int,\n","                 emb_size: int,\n","                 nhead: int,\n","                 vocab_size: int,\n","                 num_categories: int,\n","                 dim_feedforward: int = 2048,\n","                 dropout: float = 0.1,\n","                 max_input_len: int = 14,\n","                 num_decoder_layers=4):\n","        super(DescriptionTransformer, self).__init__()\n","        self.max_input_len = max_input_len\n","        self.embedding = nn.Embedding(vocab_size, emb_size)\n","\n","        # self.encoder_layer = TransformerEncoderLayer(d_model=emb_size,\n","        #                                                 nhead=nhead,\n","        #                                                 dim_feedforward=dim_feedforward,\n","        #                                                 dropout=dropout,\n","        #                                                 batch_first=True)\n","        # self.encoder = TransformerEncoder(self.encoder_layer, num_layers=num_encoder_layers)\n","        # self.flatten = nn.Flatten(start_dim=1)\n","        # self.fc = nn.Linear(max_input_len*emb_size, 512)\n","        # self.drop = nn.Dropout(0.5)\n","        # self.generator = nn.Linear(512, num_categories)\n","\n","        self.tgt_emb = nn.Embedding(1, emb_size)\n","        self.transform = Transformer(d_model=emb_size,\n","                                     nhead=nhead,\n","                                     num_encoder_layers=num_encoder_layers,\n","                                     num_decoder_layers=num_decoder_layers,\n","                                     dim_feedforward=dim_feedforward,\n","                                     dropout=dropout,\n","                                     batch_first=True)\n","        self.transform_fc = nn.Linear(emb_size, 64)\n","        self.transform_drop = nn.Dropout(0.45)\n","        self.transform_gen = nn.Linear(64, num_categories)\n","\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self,\n","                src: Tensor,\n","                src_key_padding_mask: Tensor = None):\n","        src_emb = self.embedding(src)\n","\n","        # outs = self.encoder(src_emb, src_key_padding_mask=src_key_padding_mask)\n","        \n","        # if outs.shape[1] < self.max_input_len:\n","        #     diff = self.max_input_len - outs.shape[1]\n","        #     pad = torch.zeros((outs.shape[0], diff, outs.shape[2])).to(device)\n","        #     outs = torch.cat((outs, pad), axis=1)\n","\n","        # outs = self.flatten(outs)\n","        # outs = self.drop(outs)\n","        # outs = self.fc(outs)\n","        # outs = self.drop(outs)\n","        # outs = self.generator(outs)\n","\n","        tgt = torch.zeros((src.shape[0], 1), dtype=torch.long).to(device)\n","        tgt_emb = self.tgt_emb(tgt)\n","        tgt_key_padding_mask = torch.tensor([[False]]*src.shape[0]).to(device)\n","        outs = self.transform(\n","            src_emb,\n","            tgt_emb,\n","            src_key_padding_mask=src_key_padding_mask,\n","        )\n","        outs = outs.squeeze(dim=1)\n","        outs = self.transform_fc(outs)\n","        outs = self.transform_drop(outs)\n","        outs = self.transform_gen(outs)\n","\n","        outs = self.softmax(outs)\n","        return outs"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"JW-PoyiX9Coj","executionInfo":{"status":"ok","timestamp":1682199449618,"user_tz":240,"elapsed":4,"user":{"displayName":"Patrick Wang","userId":"09514208081734500853"}}},"outputs":[],"source":["def train_epoch(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    train_loss, correct = 0, 0\n","\n","    model.train()\n","    for _, (X, y) in enumerate(dataloader):\n","        X, y = X.to(device), y.to(device)\n","\n","        mask = torch.tensor([[False if x > 0 else True for x in r] for r in X]).to(device)\n","\n","        pred = model(src=X, src_key_padding_mask=mask)\n","        loss = loss_fn(pred, y)\n","        train_loss += loss.item()\n","        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    average_train_loss = train_loss / num_batches\n","    average_train_accuracy = correct / size\n","    return average_train_accuracy, average_train_loss\n","\n","def test_epoch(dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    test_loss, correct = 0, 0\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            X, y = X.to(device), y.to(device)\n","\n","            mask = torch.tensor([[False if x > 0 else True for x in r] for r in X]).to(device)\n","\n","            pred = model(src=X, src_key_padding_mask=mask)\n","            test_loss += loss_fn(pred, y).item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","    average_test_loss = test_loss / num_batches\n","    average_test_accuracy = correct / size\n","    return average_test_accuracy, average_test_loss\n","\n","def train_full(train_dataloader, val_dataloader, model, loss_fn, optimizer, epochs=10, save_weights=False):\n","    train_accuracies, val_accuracies = [], []\n","    top_val_accuracy = 0.0\n","\n","    for t in tqdm(range(epochs)):\n","        train_accuracy, train_loss = train_epoch(train_dataloader, model, loss_fn, optimizer)\n","        train_accuracies += [train_accuracy]\n","\n","        val_accuracy, val_loss = test_epoch(val_dataloader, model, loss_fn)\n","        val_accuracies += [val_accuracy]\n","\n","        if val_accuracy > top_val_accuracy:\n","            top_val_accuracy = val_accuracy\n","            if save_weights:\n","                torch.save(model, 'desc_model.pth')\n","\n","        print(f\"Epoch {t+1}:\\t Train accuracy: {100*train_accuracy:0.1f}%\\t Avg train loss: {train_loss:>6f}\\t Val accuracy: {100*val_accuracy:0.1f}%\\t Avg val loss: {val_loss:>6f}\")\n","\n","    print(f\"Top val accuracy: {top_val_accuracy}\")\n","    return train_accuracies, val_accuracies"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kAxGWU-3_tg3","executionInfo":{"status":"ok","timestamp":1682199819168,"user_tz":240,"elapsed":369554,"user":{"displayName":"Patrick Wang","userId":"09514208081734500853"}},"outputId":"ad4d270b-6792-4ac9-f881-ea2e80e0391e"},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/torch/nn/modules/transformer.py:287: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:177.)\n","  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n","  5%|▌         | 1/20 [00:21<06:41, 21.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1:\t Train accuracy: 41.6%\t Avg train loss: 2.247027\t Val accuracy: 50.4%\t Avg val loss: 1.863769\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 2/20 [00:39<05:52, 19.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 2:\t Train accuracy: 51.9%\t Avg train loss: 1.834209\t Val accuracy: 56.8%\t Avg val loss: 1.664369\n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▌        | 3/20 [00:57<05:18, 18.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 3:\t Train accuracy: 57.7%\t Avg train loss: 1.652552\t Val accuracy: 66.6%\t Avg val loss: 1.441211\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 4/20 [01:15<04:56, 18.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 4:\t Train accuracy: 61.9%\t Avg train loss: 1.522529\t Val accuracy: 69.0%\t Avg val loss: 1.335737\n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▌       | 5/20 [01:33<04:34, 18.30s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 5:\t Train accuracy: 65.3%\t Avg train loss: 1.411646\t Val accuracy: 72.0%\t Avg val loss: 1.235521\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 6/20 [01:51<04:16, 18.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 6:\t Train accuracy: 67.7%\t Avg train loss: 1.339058\t Val accuracy: 73.0%\t Avg val loss: 1.164066\n"]},{"output_type":"stream","name":"stderr","text":["\r 35%|███▌      | 7/20 [02:09<03:55, 18.15s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 7:\t Train accuracy: 69.9%\t Avg train loss: 1.260106\t Val accuracy: 74.5%\t Avg val loss: 1.139568\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 8/20 [02:27<03:37, 18.13s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 8:\t Train accuracy: 71.8%\t Avg train loss: 1.213452\t Val accuracy: 77.9%\t Avg val loss: 1.081710\n"]},{"output_type":"stream","name":"stderr","text":["\r 45%|████▌     | 9/20 [02:45<03:18, 18.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 9:\t Train accuracy: 73.2%\t Avg train loss: 1.169836\t Val accuracy: 79.4%\t Avg val loss: 1.023419\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 10/20 [03:03<03:00, 18.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 10:\t Train accuracy: 74.5%\t Avg train loss: 1.131493\t Val accuracy: 79.8%\t Avg val loss: 0.986460\n"]},{"output_type":"stream","name":"stderr","text":["\r 55%|█████▌    | 11/20 [03:21<02:42, 18.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 11:\t Train accuracy: 76.0%\t Avg train loss: 1.101406\t Val accuracy: 80.8%\t Avg val loss: 0.922726\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 12/20 [03:39<02:23, 18.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 12:\t Train accuracy: 76.4%\t Avg train loss: 1.072780\t Val accuracy: 81.3%\t Avg val loss: 0.928944\n"]},{"output_type":"stream","name":"stderr","text":["\r 65%|██████▌   | 13/20 [03:57<02:06, 18.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 13:\t Train accuracy: 76.6%\t Avg train loss: 1.058218\t Val accuracy: 80.7%\t Avg val loss: 0.928622\n"]},{"output_type":"stream","name":"stderr","text":["\r 70%|███████   | 14/20 [04:15<01:47, 17.96s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 14:\t Train accuracy: 77.5%\t Avg train loss: 1.031319\t Val accuracy: 81.8%\t Avg val loss: 0.907492\n"]},{"output_type":"stream","name":"stderr","text":["\r 75%|███████▌  | 15/20 [04:34<01:30, 18.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 15:\t Train accuracy: 78.5%\t Avg train loss: 1.008666\t Val accuracy: 82.9%\t Avg val loss: 0.881846\n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 16/20 [04:52<01:12, 18.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 16:\t Train accuracy: 78.7%\t Avg train loss: 0.992611\t Val accuracy: 83.3%\t Avg val loss: 0.830994\n"]},{"output_type":"stream","name":"stderr","text":["\r 85%|████████▌ | 17/20 [05:10<00:54, 18.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 17:\t Train accuracy: 79.0%\t Avg train loss: 0.993157\t Val accuracy: 83.5%\t Avg val loss: 0.850338\n"]},{"output_type":"stream","name":"stderr","text":["\r 90%|█████████ | 18/20 [05:28<00:35, 17.99s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 18:\t Train accuracy: 78.9%\t Avg train loss: 0.993352\t Val accuracy: 82.4%\t Avg val loss: 0.859844\n"]},{"output_type":"stream","name":"stderr","text":["\r 95%|█████████▌| 19/20 [05:46<00:18, 18.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 19:\t Train accuracy: 79.0%\t Avg train loss: 0.977918\t Val accuracy: 81.6%\t Avg val loss: 0.868281\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 20/20 [06:04<00:00, 18.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Epoch 20:\t Train accuracy: 79.4%\t Avg train loss: 0.969861\t Val accuracy: 83.3%\t Avg val loss: 0.833192\n","Top val accuracy: 0.8349514563106796\n","CPU times: user 6min, sys: 2.53 s, total: 6min 2s\n","Wall time: 6min 9s\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["%%time\n","\n","# desc_model = DescriptionTransformer(num_encoder_layers=6,\n","#                                 emb_size=256,\n","#                                 nhead=4,\n","#                                 vocab_size=num_words,\n","#                                 num_categories=num_categories,\n","#                                 dim_feedforward=256,\n","#                                 dropout=0.6,\n","#                                 max_input_len=max_desc_len).to(device)\n","\n","# desc_model = DescriptionTransformer(num_encoder_layers=6,\n","#                                 emb_size=128,\n","#                                 nhead=4,\n","#                                 vocab_size=num_words,\n","#                                 num_categories=num_categories,\n","#                                 dim_feedforward=512,\n","#                                 dropout=0.45,\n","#                                 max_input_len=max_desc_len).to(device)\n","\n","desc_model = DescriptionTransformer(num_encoder_layers=4,\n","                                emb_size=128,\n","                                nhead=2,\n","                                vocab_size=num_words,\n","                                num_categories=num_categories,\n","                                dim_feedforward=64,\n","                                dropout=0.45,\n","                                max_input_len=max_desc_len,\n","                                num_decoder_layers=2).to(device)\n","\n","save_weights = True\n","load_weights = False\n","num_epochs = 20\n","\n","if load_weights:\n","    desc_model = torch.load('desc_model.pth')\n","\n","loss_fn = nn.NLLLoss()\n","optimizer = torch.optim.Adam(desc_model.parameters())\n","\n","# train_accuracies, val_accuracies = train_full(train_dataloader, val_dataloader, desc_model, loss_fn, optimizer, epochs=num_epochs, save_weights=save_weights)\n","train_accuracies, val_accuracies = train_full(total_dataloader, val_dataloader, desc_model, loss_fn, optimizer, epochs=num_epochs, save_weights=save_weights)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"UP14Zjv_ipHs","executionInfo":{"status":"ok","timestamp":1682199819337,"user_tz":240,"elapsed":172,"user":{"displayName":"Patrick Wang","userId":"09514208081734500853"}}},"outputs":[],"source":["df = pd.read_csv('train.csv')[['category', 'noisyTextDescription']]\n","\n","df['noisyTextDescription'] = df['noisyTextDescription'].apply(convert_text)\n","df['category'] = df['category'].apply(lambda x: c_to_i[x])"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"AF6ecgSKillG","executionInfo":{"status":"ok","timestamp":1682199819338,"user_tz":240,"elapsed":3,"user":{"displayName":"Patrick Wang","userId":"09514208081734500853"}}},"outputs":[],"source":["if save_weights or load_weights:\n","    desc_model = torch.load('desc_model.pth')\n","\n","desc_model.eval()\n","\n","def make_predictions(df):\n","    eval_dataset = DescriptionData(df, test=True)\n","    eval_dataloader = DataLoader(eval_dataset, batch_size=128, shuffle=False)\n","\n","    predictions = []\n","    for _, X in enumerate(eval_dataloader):\n","        X = X.to(device)\n","\n","        mask = torch.tensor([[False if x > 0 else True for x in r] for r in X]).to(device)\n","\n","        pred = desc_model(src=X, src_key_padding_mask=mask)\n","        labels = pred.argmax(1)\n","        for j in range(pred.shape[0]):\n","            predictions.append(i_to_c[labels[j].item()])\n","\n","    return predictions\n","\n","def eval_pred(base, pred):\n","    assert(base['id'].equals(pred['id']))\n","    print('ids match')\n","    diff_count = (base['category'] == pred['category']).value_counts()\n","\n","    if True not in diff_count:\n","        print(\"WTF\")\n","        return 0.0\n","    else:\n","        return (100.0*diff_count[True])/len(base)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21772,"status":"ok","timestamp":1682199841108,"user":{"displayName":"Patrick Wang","userId":"09514208081734500853"},"user_tz":240},"id":"NPmmBgR6jtlG","outputId":"c329bd2e-8016-494c-a520-9900505da85c"},"outputs":[{"output_type":"stream","name":"stderr","text":["<timed exec>:8: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]},{"output_type":"stream","name":"stdout","text":["ids match\n","train accuracy: 83.02584732047903\n","CPU times: user 21.6 s, sys: 67.8 ms, total: 21.7 s\n","Wall time: 21.8 s\n"]},{"output_type":"stream","name":"stderr","text":["<timed exec>:16: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"]}],"source":["%%time\n","\n","test_features = ['noisyTextDescription']\n","\n","df = pd.read_csv('train.csv')\n","\n","df['noisyTextDescription'] = df['noisyTextDescription'].apply(convert_text)\n","\n","train_pred = df[['id']]\n","train_pred['category'] = make_predictions(df)\n","\n","print(f'train accuracy: {eval_pred(df, train_pred)}')\n","\n","df_test = pd.read_csv('test.csv')\n","df_test['noisyTextDescription'] = df_test['noisyTextDescription'].apply(convert_text)\n","\n","test_pred = df_test[['id']]\n","test_pred['category'] = make_predictions(df_test)\n","assert(df_test['id'].equals(test_pred['id']))"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"1TvKcFNOKXpY","executionInfo":{"status":"ok","timestamp":1682199841109,"user_tz":240,"elapsed":21,"user":{"displayName":"Patrick Wang","userId":"09514208081734500853"}}},"outputs":[],"source":["train_pred.to_csv('desc_train_pred.csv', index=False)\n","test_pred.to_csv('desc_test_pred.csv', index=False)"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"ura","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}